<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Gene Chou</title>

  <meta name="author" content="Gene Chou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="CS PhD student at Cornell, advised by Noah Snavely and Bharath Hariharan.">


  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/cornell_seal.png">
</head>

<body>
  <table
    style="width:100%;max-width:1080px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:left">
                    <name>Gene Chou</name>
                  </p>
                  <p> I am a second year CS PhD student at Cornell University, advised by <a href="https://www.cs.cornell.edu/~snavely/" target="_blank" rel="noopener noreferrer">Noah Snavely</a> and <a href="http://home.bharathh.info" target="_blank" rel="noopener noreferrer">Bharath Hariharan</a>. 
                    Previously, I received my bachelor's in computer science and applied math from Princeton University and worked with <a href="https://www.cs.princeton.edu/~fheide/" target="_blank" rel="noopener noreferrer">Felix Heide</a>. 
                    My research focuses on 3D/4D reconstruction and generation. I am supported by the NSF graduate fellowship.
                   </p>
                  <!-- begone bot -->
                  <p><b>Contact:</b> gene@cs.cornell.edu</p>

                  <!-- <p style="text-align:center"> -->
                    <!-- <a href="index.html#CV">cv</a> &nbsp/&nbsp -->
                    <a href="GeneChouCV.pdf">cv</a> &nbsp/&nbsp 
                    <a href="https://github.com/gene-chou/" target="_blank" rel="noopener noreferrer">github</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/gene-chou" target="_blank" rel="noopener noreferrer">linkedin &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=FZN-I0QAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">google scholar &nbsp/&nbsp
                    <a href="https://photos.app.goo.gl/hP1RqUh3V7avxSG79" target="_blank" rel="noopener noreferrer">pictures of my cat</a> 
                  <!-- </p> -->
                </td>

                <!-- PROFILE -->
                <td style="padding:2%;width:20%;vertical-align:bottom">
                    <img src='images/profile-square.png' width="90%">              
                </td>

              </tr>
            </tbody>
          </table>
          <br>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:top">
                  <heading>Research</heading>
                  <p>
                    <!-- research -->
                  </p>
                  <hr>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- BEGIN PAPERS -->
              <tr>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <img src='images/flashdepth_teaser.gif' width="70%"> <!-- CHANGE -->  
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <papertitle>FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  <strong>Gene Chou</strong>, Wenqi Xian, Guandao Yang, Mohamed Abdelfattah, Bharath Hariharan, Noah Snavely, Ning Yu, Paul Debevec
                  <br>
                  <em>ICCV 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2504.07093" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://github.com/Eyeline-Research/flashdepth" target="_blank" rel="noopener noreferrer">code</a> / <a href="https://eyeline-research.github.io/FlashDepth/" target="_blank" rel="noopener noreferrer">project page</a> 
                  <br>
                  <p></p>
                  <p>
                    We present FlashDepth, a video depth estimation model that processes high-resolution streaming videos in real-time (2044Ã—1148 at 24 FPS on an A100 GPU).
                  </p>
                </td>
              </tr>



              <tr>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <img src='images/kfc-teaser.gif' width="70%"> <!-- CHANGE -->  
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <papertitle>Generating 3D-Consistent Videos from Unposed Internet Photos
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  <strong>Gene Chou</strong>, Kai Zhang, Sai Bi, Hao Tan, Zexiang Xu, Fujun Luan, Bharath Hariharan, Noah Snavely
                  <br>
                  <em>CVPR 2025</em>
                  <br>
                  <a href="https://arxiv.org/abs/2411.13549" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://genechou.com/kfcw" target="_blank" rel="noopener noreferrer">project page</a>
                  <br>
                  <p></p>
                  <p>
                    We propose the task of generating videos from sparse, unposed internet photos, 
                    and design a self-supervised method that takes advantage of the consistency of videos and 
                    variability of multiview internet photos to train a 3D-aware video model without any 3D annotations such as camera parameters.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <img src='images/megascenes_teaser.png' width="75%"> <!-- CHANGE -->  
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <papertitle>MegaScenes: Scene-Level View Synthesis at Scale
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  Joseph Tung*, <strong>Gene Chou</strong>*, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, Noah Snavely
                  <br>
                  <em>ECCV 2024</em>
                  <br>
                  <a href="https://arxiv.org/abs/2406.11819" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://github.com/MegaScenes/nvs" target="_blank" rel="noopener noreferrer">code</a> / <a href="https://megascenes.github.io/" target="_blank" rel="noopener noreferrer">project page</a>
                  <br>
                  <p></p>
                  <p>
                    MegaScenes is a scene-level dataset containing 100K SfM reconstructions and 2M registered images, collected from Wikimedia Commons. 
                    We validate its effectiveness in training large-scale, generalizable models on the task of single image novel view synthesis.
                  </p>
                </td>
              </tr>
              <!-- /PAPER -->

              <!-- PAPER -->
              <tr>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <img src='images/yolor_mt.png' width="75%"> <!-- CHANGE -->  
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <papertitle>Generalist YOLO: Towards Real-Time End-to-End Multi-Task Visual Language Models
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  Hung-Shuo Chang, Chien-Yao Wang, Richard Wang, <strong>Gene Chou</strong>, Hong-Yuan Mark Liao
                  <br>
                  <em>WACV 2025</em>
                  <br>
                  <a href="https://openaccess.thecvf.com/content/WACV2025/papers/Chang_Generalist_YOLO_Towards_Real-Time_End-to-End_Multi-Task_Visual_Language_Models_WACV_2025_paper.pdf" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://github.com/WongKinYiu/GeneralistYOLO" target="_blank" rel="noopener noreferrer">code</a>
                  <br>
                  <p></p>
                  <p>
                    Builds on YOLOR to jointly train vision (e.g. object detection, instance and semantic segmentation) and vision-language (e.g. image captioning) tasks. Fast and lightweight while achieving competitive performance. 
                  </p>
                </td>
                </tr>
              <!-- /PAPER -->

              <!-- PAPER -->
              <tr>
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <img src='images/nanoarray1.png' width="100%"> <!-- CHANGE -->  
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <!-- <a href="https://github.com/princeton-computational-imaging/gensdf"> -->
                  <papertitle>Thin On-Sensor Nanophotonic Array Cameras
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  Praneeth Chakravarthula, Jipeng Sun, Xiao Li, Chenyang Lei, <strong>Gene Chou</strong>, Mario Bijelic, Johannes Froesch, Arka Majumdar, Felix Heide
                  <br>
                  <em>SIGGRAPH ASIA 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2308.02797" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://light.princeton.edu/publication/thin-on-sensor-nanophotonic-array-cameras/" target="_blank" rel="noopener noreferrer">project page</a>
                  <br>
                  <p></p>
                  <p>
                    Recovers images in broadband using a single flat metasurface optic. Compensates for residual aberrations with probabilistic deconvolution implemented using a conditional diffusion model.
                  </p>
                </td>
                </tr>
              <!-- /PAPER -->

              <!-- PAPER -->
              <tr onmouseout="diffusionsdf_stop()" onmouseover="diffusionsdf_start()">
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='diffusionsdf'>
                      <img src='images/diffusionsdf_before.png' width="100%">
                    </div> <!-- CHANGE -->
                    <img src='images/diffusionsdf_before.png' width="100%"> <!-- CHANGE -->
                  </div>
                  <script type="text/javascript">
                    function diffusionsdf_start() {
                      document.getElementById('diffusionsdf').style.opacity = "1";
                    }

                    function diffusionsdf_stop() {
                      document.getElementById('diffusionsdf').style.opacity = "0";
                    }
                    diffusionsdf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <!-- <a href="https://github.com/princeton-computational-imaging/gensdf"> -->
                  <papertitle>Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  <strong>Gene Chou</strong>, Yuval Bahat, Felix Heide
                  <!-- <a href="https://sites.google.com/view/yuval-bahat/home" target="_blank" rel="noopener noreferrer">Yuval Bahat</a>,
                  <a href="https://www.cs.princeton.edu/~fheide/" target="_blank" rel="noopener noreferrer">Felix Heide</a> -->
                  <br>
                  <em>ICCV 2023</em>
                  <br>
                  <a href="https://arxiv.org/abs/2211.13757" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://github.com/princeton-computational-imaging/Diffusion-SDF" target="_blank" rel="noopener noreferrer">code</a> / <a href="https://light.princeton.edu/publication/diffusionsdf/" target="_blank" rel="noopener noreferrer">project page</a> 
                  <!-- / <a href="https://drive.google.com/file/d/11VKV-GiyDOTv3OuyW3Y3F5-Y7WLwMjj8/view?usp=share_link" target="_blank" rel="noopener noreferrer">teaser video</a>   -->
                  <!-- [<a href="https://arxiv.org/pdf/2206.02780.pdf">Paper</a>] [<a href="https://github.com/princeton-computational-imaging/gensdf">Code</a>] [<a href="https://light.princeton.edu/publication/gensdf/">Project page</a>] -->
                  <br>
                  <p></p>
                  <p>
                    Performs diffusion on the latent space of neural SDFs while providing geometric guidance. Generates diverse meshes conditioned on partial point clouds, 2D images, and real-scanned, noisy point clouds.
                  </p>
                </td>
              </tr>
              <!-- /PAPER -->

              <!-- PAPER -->
              <tr onmouseout="gensdf_stop()" onmouseover="gensdf_start()">
                <td style="padding:2.5%;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gensdf'>
                      <img src='images/gensdf_after.png' width="90%">
                    </div> <!-- CHANGE -->
                    <img src='images/gensdf_before.png' width="90%"> <!-- CHANGE -->
                  </div>
                  <script type="text/javascript">
                    function gensdf_start() {
                      document.getElementById('gensdf').style.opacity = "1";
                    }

                    function gensdf_stop() {
                      document.getElementById('gensdf').style.opacity = "0";
                    }
                    gensdf_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle;">
                  <!-- <a href="https://github.com/princeton-computational-imaging/gensdf"> -->
                  <papertitle>GenSDF: Two-Stage Learning of Generalizable Signed Distance Functions
                  </papertitle>
                  <!-- </a> -->
                  <br>
                  <strong>Gene Chou</strong>, Ilya Chugunov, Felix Heide
                  <!-- <a href="https://ilyac.info" target="_blank" rel="noopener noreferrer">Ilya Chugunov</a>,
                  <a href="https://www.cs.princeton.edu/~fheide/" target="_blank" rel="noopener noreferrer">Felix Heide</a> -->
                  <br>
                  <em>NeurIPS 2022</em>
                  <br>
                  <a href="https://arxiv.org/abs/2206.02780" target="_blank" rel="noopener noreferrer">paper</a> / <a href="https://github.com/princeton-computational-imaging/gensdf" target="_blank" rel="noopener noreferrer">code</a> / <a href="https://light.princeton.edu/publication/gensdf/" target="_blank" rel="noopener noreferrer">project page</a>
                  <br>
                  <p></p>
                  <p>
                    Combines a semi-supervised approach with a self-supervised loss to reconstruct neural SDFs from raw input point clouds of over a hundred unseen object classes.

                  </p>
                </td>
              </tr>
              <!-- /PAPER -->

              <!-- CV -->
              <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                  <a name='CV'></a>
                  <tr>
                    <td>
                      <heading> CV </heading>
                      <hr>
                      <p align="center"><iframe src="GeneChouCV.pdf" width="85%" height="1000vh" style="border: none;" allow="download"></iframe></p>

                    </td>
                  </tr>
                </tbody>
              </table> -->


              <!-- \CV -->

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>



                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:8pt;color: gray;">
                        Website template borrowed from <a href="https://jonbarron.info/" , style="font-size: 8pt" target="_blank" rel="noopener noreferrer">Jon
                          Barron</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>
